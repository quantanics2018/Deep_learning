{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayed-Husain/Image-Classification-Projects/blob/main/Fresh%20Rotten%20Fruits/fruits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HIAwTmDVXYy"
      },
      "source": [
        "# Fresh and Rotten Fruits Image Classification with Convolutional Neural Networks from Scratch using TensorFlow and Keras\n",
        "\n",
        "\n",
        "The objective of the project is to develop a robust image classification system capable of distinguishing between fresh and rotten fruits.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset or use kaggle API key for access the dataset #dataset link - https://www.kaggle.com/datasets/sriramr/fruits-fresh-and-rotten-for-classification"
      ],
      "metadata": {
        "id": "5U8mo8FC-wW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FtpTUsq0_zi"
      },
      "outputs": [],
      "source": [
        "\n",
        "!unzip /content/fruits-fresh-and-rotten-for-classification.zip -d /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct6aRgSRV5rS"
      },
      "source": [
        "## Load ImageNet Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2xaepzqaYFqb",
        "outputId": "8339af3f-993f-46b8-e0eb-d1d9b5498961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "base_model = keras.applications.VGG16(\n",
        "    weights='imagenet',\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "stdGF0a6YGAs"
      },
      "outputs": [],
      "source": [
        "# Freeze base model\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc0BlNHaYIXd"
      },
      "source": [
        "## Add Layers to Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ex4Ptx0JYJlY"
      },
      "outputs": [],
      "source": [
        "# Create inputs with correct shape\n",
        "inputs = keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# Add pooling layer or flatten layer\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add final dense layer\n",
        "outputs = keras.layers.Dense(6, activation = 'softmax')(x)\n",
        "\n",
        "# Combine inputs and outputs to create model\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t095en-KYNTw",
        "outputId": "947f2f21-3290-4bd0-a032-29a317e903de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 512)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14717766 (56.14 MB)\n",
            "Trainable params: 3078 (12.02 KB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjYag64HYPiT"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DWUSFhGQYRkG"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUX0dfmtYdjD"
      },
      "source": [
        "## Augment the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Er-oJXEhYgAT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        samplewise_center=True,  # set each sample mean to 0\n",
        "        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD2Tuv3mYjhU"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CC7RKpu1Yjxq",
        "outputId": "788d97e1-f4b7-40c3-e1bb-08586950b15f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10901 images belonging to 6 classes.\n",
            "Found 2698 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "# load and iterate training dataset\n",
        "train_it = datagen.flow_from_directory('/content/dataset/train',\n",
        "                                       target_size=(224, 224),\n",
        "                                       color_mode='rgb',\n",
        "                                       class_mode=\"categorical\")\n",
        "# load and iterate validation dataset\n",
        "valid_it = datagen.flow_from_directory('/content/dataset/test',\n",
        "                                      target_size=(224, 224),\n",
        "                                      color_mode='rgb',\n",
        "                                      class_mode=\"categorical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwGP2R7QYnQH"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OfmxXjLpYopt",
        "outputId": "9209e7cb-7225-4777-e0fd-6843558c9020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "340/340 [==============================] - 273s 759ms/step - loss: 0.9758 - accuracy: 0.7642 - val_loss: 0.2750 - val_accuracy: 0.9018\n",
            "Epoch 2/30\n",
            "340/340 [==============================] - 255s 748ms/step - loss: 0.2100 - accuracy: 0.9313 - val_loss: 0.1622 - val_accuracy: 0.9396\n",
            "Epoch 3/30\n",
            "340/340 [==============================] - 218s 639ms/step - loss: 0.1348 - accuracy: 0.9542 - val_loss: 0.1284 - val_accuracy: 0.9592\n",
            "Epoch 4/30\n",
            "340/340 [==============================] - 216s 634ms/step - loss: 0.0975 - accuracy: 0.9656 - val_loss: 0.0939 - val_accuracy: 0.9711\n",
            "Epoch 5/30\n",
            "340/340 [==============================] - 217s 637ms/step - loss: 0.0893 - accuracy: 0.9662 - val_loss: 0.0941 - val_accuracy: 0.9674\n",
            "Epoch 6/30\n",
            "340/340 [==============================] - 217s 636ms/step - loss: 0.0775 - accuracy: 0.9723 - val_loss: 0.0830 - val_accuracy: 0.9729\n",
            "Epoch 7/30\n",
            "340/340 [==============================] - 216s 634ms/step - loss: 0.0690 - accuracy: 0.9758 - val_loss: 0.0797 - val_accuracy: 0.9715\n",
            "Epoch 8/30\n",
            "340/340 [==============================] - 231s 680ms/step - loss: 0.0613 - accuracy: 0.9775 - val_loss: 0.0608 - val_accuracy: 0.9748\n",
            "Epoch 9/30\n",
            "340/340 [==============================] - 262s 769ms/step - loss: 0.0573 - accuracy: 0.9801 - val_loss: 0.0903 - val_accuracy: 0.9692\n",
            "Epoch 10/30\n",
            "340/340 [==============================] - 218s 640ms/step - loss: 0.0533 - accuracy: 0.9815 - val_loss: 0.0686 - val_accuracy: 0.9729\n",
            "Epoch 11/30\n",
            "340/340 [==============================] - 217s 636ms/step - loss: 0.0541 - accuracy: 0.9814 - val_loss: 0.0889 - val_accuracy: 0.9670\n",
            "Epoch 12/30\n",
            "340/340 [==============================] - 219s 642ms/step - loss: 0.0607 - accuracy: 0.9794 - val_loss: 0.0539 - val_accuracy: 0.9822\n",
            "Epoch 13/30\n",
            "340/340 [==============================] - 219s 642ms/step - loss: 0.0440 - accuracy: 0.9845 - val_loss: 0.0596 - val_accuracy: 0.9778\n",
            "Epoch 14/30\n",
            "340/340 [==============================] - 219s 644ms/step - loss: 0.0501 - accuracy: 0.9822 - val_loss: 0.0736 - val_accuracy: 0.9770\n",
            "Epoch 15/30\n",
            "340/340 [==============================] - 218s 641ms/step - loss: 0.0484 - accuracy: 0.9835 - val_loss: 0.0444 - val_accuracy: 0.9841\n",
            "Epoch 16/30\n",
            "340/340 [==============================] - 216s 633ms/step - loss: 0.0405 - accuracy: 0.9868 - val_loss: 0.0732 - val_accuracy: 0.9755\n",
            "Epoch 17/30\n",
            "340/340 [==============================] - 217s 636ms/step - loss: 0.0419 - accuracy: 0.9847 - val_loss: 0.0592 - val_accuracy: 0.9830\n",
            "Epoch 18/30\n",
            "340/340 [==============================] - 258s 756ms/step - loss: 0.0486 - accuracy: 0.9838 - val_loss: 0.0611 - val_accuracy: 0.9804\n",
            "Epoch 19/30\n",
            "340/340 [==============================] - 216s 634ms/step - loss: 0.0514 - accuracy: 0.9817 - val_loss: 0.0477 - val_accuracy: 0.9837\n",
            "Epoch 20/30\n",
            "340/340 [==============================] - 215s 630ms/step - loss: 0.0406 - accuracy: 0.9857 - val_loss: 0.0535 - val_accuracy: 0.9807\n",
            "Epoch 21/30\n",
            "340/340 [==============================] - 214s 629ms/step - loss: 0.0418 - accuracy: 0.9860 - val_loss: 0.0551 - val_accuracy: 0.9830\n",
            "Epoch 22/30\n",
            "340/340 [==============================] - 214s 629ms/step - loss: 0.0377 - accuracy: 0.9872 - val_loss: 0.0537 - val_accuracy: 0.9807\n",
            "Epoch 23/30\n",
            "340/340 [==============================] - 207s 608ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: 0.0491 - val_accuracy: 0.9855\n",
            "Epoch 24/30\n",
            "340/340 [==============================] - 206s 605ms/step - loss: 0.0419 - accuracy: 0.9865 - val_loss: 0.0799 - val_accuracy: 0.9766\n",
            "Epoch 25/30\n",
            "340/340 [==============================] - 208s 612ms/step - loss: 0.0313 - accuracy: 0.9890 - val_loss: 0.0466 - val_accuracy: 0.9855\n",
            "Epoch 26/30\n",
            "340/340 [==============================] - 208s 611ms/step - loss: 0.0368 - accuracy: 0.9868 - val_loss: 0.0453 - val_accuracy: 0.9852\n",
            "Epoch 27/30\n",
            "340/340 [==============================] - 208s 610ms/step - loss: 0.0369 - accuracy: 0.9873 - val_loss: 0.0513 - val_accuracy: 0.9855\n",
            "Epoch 28/30\n",
            "340/340 [==============================] - 208s 610ms/step - loss: 0.0393 - accuracy: 0.9868 - val_loss: 0.0584 - val_accuracy: 0.9815\n",
            "Epoch 29/30\n",
            "340/340 [==============================] - 208s 611ms/step - loss: 0.0340 - accuracy: 0.9867 - val_loss: 0.0397 - val_accuracy: 0.9893\n",
            "Epoch 30/30\n",
            "340/340 [==============================] - 210s 617ms/step - loss: 0.0331 - accuracy: 0.9886 - val_loss: 0.0568 - val_accuracy: 0.9818\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7eaea062a020>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.fit(train_it,\n",
        "          validation_data=valid_it,\n",
        "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
        "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
        "          epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6oB8guNYp0f"
      },
      "source": [
        "## Unfreeze Model for Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i6oAN--3Yr0G"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Compile the model with a low learning rate\n",
        "model.compile(loss = \"categorical_crossentropy\",\n",
        "              optimizer = keras.optimizers.RMSprop(learning_rate = 0.00005),\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uKMrty8dYzKa",
        "outputId": "65edf409-1247-429f-ecd7-19cefa883652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/45\n",
            "340/340 [==============================] - 285s 792ms/step - loss: 0.3299 - accuracy: 0.9275 - val_loss: 0.0241 - val_accuracy: 0.9907\n",
            "Epoch 32/45\n",
            "340/340 [==============================] - 259s 759ms/step - loss: 0.1151 - accuracy: 0.9712 - val_loss: 0.0113 - val_accuracy: 0.9956\n",
            "Epoch 33/45\n",
            "340/340 [==============================] - 254s 745ms/step - loss: 0.1007 - accuracy: 0.9807 - val_loss: 0.0058 - val_accuracy: 0.9981\n",
            "Epoch 34/45\n",
            "340/340 [==============================] - 257s 754ms/step - loss: 0.1101 - accuracy: 0.9805 - val_loss: 0.0111 - val_accuracy: 0.9959\n",
            "Epoch 35/45\n",
            "340/340 [==============================] - 258s 758ms/step - loss: 0.0720 - accuracy: 0.9850 - val_loss: 0.0152 - val_accuracy: 0.9963\n",
            "Epoch 36/45\n",
            "340/340 [==============================] - 256s 752ms/step - loss: 0.1070 - accuracy: 0.9847 - val_loss: 0.3401 - val_accuracy: 0.9581\n",
            "Epoch 37/45\n",
            "340/340 [==============================] - 255s 747ms/step - loss: 0.0905 - accuracy: 0.9835 - val_loss: 0.0588 - val_accuracy: 0.9878\n",
            "Epoch 38/45\n",
            "340/340 [==============================] - 265s 777ms/step - loss: 0.1164 - accuracy: 0.9841 - val_loss: 0.0227 - val_accuracy: 0.9930\n",
            "Epoch 39/45\n",
            "340/340 [==============================] - 259s 759ms/step - loss: 0.0975 - accuracy: 0.9850 - val_loss: 0.0365 - val_accuracy: 0.9918\n",
            "Epoch 40/45\n",
            "340/340 [==============================] - 250s 734ms/step - loss: 0.1182 - accuracy: 0.9812 - val_loss: 0.0408 - val_accuracy: 0.9893\n",
            "Epoch 41/45\n",
            "340/340 [==============================] - 245s 718ms/step - loss: 0.1017 - accuracy: 0.9836 - val_loss: 0.0269 - val_accuracy: 0.9941\n",
            "Epoch 42/45\n",
            "340/340 [==============================] - 248s 728ms/step - loss: 0.1570 - accuracy: 0.9807 - val_loss: 0.0238 - val_accuracy: 0.9926\n",
            "Epoch 43/45\n",
            "340/340 [==============================] - 247s 723ms/step - loss: 0.1190 - accuracy: 0.9817 - val_loss: 0.0595 - val_accuracy: 0.9889\n",
            "Epoch 44/45\n",
            "340/340 [==============================] - 249s 731ms/step - loss: 0.1625 - accuracy: 0.9802 - val_loss: 0.0167 - val_accuracy: 0.9952\n",
            "Epoch 45/45\n",
            "340/340 [==============================] - 247s 725ms/step - loss: 0.1718 - accuracy: 0.9851 - val_loss: 0.0135 - val_accuracy: 0.9974\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7eaea0629c00>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.fit(train_it,\n",
        "          validation_data=valid_it,\n",
        "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
        "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
        "          epochs=45,\n",
        "          initial_epoch=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy2cM3CuY1KV"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HWR0Bd7YY11t",
        "outputId": "ece24ad2-dcda-4d7a-b4b6-e565945c8be9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/84 [==============================] - 41s 488ms/step - loss: 0.0130 - accuracy: 0.9963\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.013014128431677818, 0.9962935447692871]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion                                                                   \n",
        "  In conclusion, the Fresh and Rotten Fruits Image Classification Convolutional Neural Network, developed from scratch using TensorFlow and Keras, achieves outstanding performance with a remarkable 99.63% accuracy and minimal categorical cross-entropy loss of 0.0130. These results underscore the model's proficiency in learning intricate patterns, confirming its effectiveness in accurately classifying fresh and rotten fruits across diverse categories."
      ],
      "metadata": {
        "id": "b7cyfMYY7Xim"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IWizwKSOXNB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}